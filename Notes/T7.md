# T7 - Hyperparameter Tuning

One way of hyper-paramter tuning is we create a dictionary known as `params` which contain a dictionary of hyperparameters.
And this `params` dictionary can be called using the `evaluate_model()` with a a parameter `params`.

Reference: 
1. [Link to sample model_trainer.py](https://github.com/krishnaik06/mlproject/blob/main/src/components/model_trainer.py)
2. [GridSearch CV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

But there is another better way using `yaml` file. 

Going with the 1st method for now.

#### Difference between these 2 lines of codes

```
model = gs.best_estimator_  # Get the best model from GridSearchCV

#### OR

model.set_params(**gs.best_params_)
```

> `model = gs.best_estimator_`

- This replaces your `model` variable with the actual best model found by `GridSearchCV`, already fitted on the training data.
- The best estimator includes all the best hyperparameters and is ready to use.

> `model.set_params(**gs.best_params_)`

- This updates the parameters of your existing model object to the best parameters found by `GridSearchCV`.
- However, this does not fit the model; you still need to call `model.fit()` after this.

##### Summary

- `model = gs.best_estimator_` is preferred and simpler: it gives you the best, already-fitted model.
- `model.set_params(**gs.best_params_)` only updates the parameters; you must fit the model again.

In most cases, use `model = gs.best_estimator_` for clarity and correctness.

On running this code in command prompt - `(myenv) D:\Github\mlproject>python -m src.components.data_ingestion`. The code runs perfectly but it also gives some warnings.

### 1. ConvergenceWarning

```
D:\Github\mlproject\myenv\Lib\site-packages\sklearn\linear_model_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.949e+02, tolerance: 1.165e+01
model = cd_fast.enet_coordinate_descent(
D:\Github\mlproject\myenv\Lib\site-packages\sklearn\linear_model_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.949e+02, tolerance: 1.165e+01
```

- __What it means:__
This warning is from scikit-learn and tells you that the optimization algorithm for a linear model (like Lasso or Ridge) did not fully converge to the optimal solution within the default number of iterations.
- __Why it happens:__
    - The data may not be well-scaled (features should be standardized).
    - The number of iterations (`max_iter`) may be too low.
    - The regularization parameter may need adjustment.
- __What to do:__
    - Increase `max_iter` in your model's parameters.
    - Make sure your features are properly scaled (which you are already doing with `StandardScaler`).
    - Consider adjusting the regularization strength.

### 2. Fitting ... folds for ... candidates

```
Fitting 3 folds for each of 12 candidates, totalling 36 fits
Fitting 3 folds for each of 32 candidates, totalling 96 fits
```

- __What it means:__
These lines are informational messages from scikit-learn's `GridSearchCV`.
    - "3 folds" means 3-fold cross-validation is being used.
    - "12 candidates" means 12 different parameter combinations are being tested.
    - "36 fits" means 12 Ã— 3 = 36 model fits will be performed.

- __Why it happens:__
This is normal and just tells you the progress of hyperparameter tuning.


6:40